## Neural Networks

### Artificial Neural Nets
- Often used for Classification: Binary and multiclass
- Can also be used for: regression, unsupervised learning
- Similar to SVM

- Timeline: advanced in 60s, slowed in 70s, advances in 80s, slowed in 90s, advanced in 2000s

### Biological Neural Networks
- ~100 neurons in human body, each connected to ~1000 others, organized in layers
- Functional classes of neurons: stimulus (perceivers), afferent (pass inwards), efferent (pass outwards), response (doers)
- I/O is electrical voltage - output is decided by summing inputs, sending output if exceeds threshold
  - Dendrite to Axon connection: synapsis - can amplify (excitatory) or reduce (inhibitory) the signal strength

### Artificial Neural Networks Overview
- **5 Tribes of Machine Learning:**
  - *Tribe      | Origins           | Master Algorithm*
  - Symbolists | Logic, Philosophy | Inverse Deduction
  - Connectionists | Neuroscience  | Backpropagation
  - Evolutionaries | Evolutionary Biology | Genetic Programming
  - Bayesians | Statistics | Probabilistic Inference
  - Analogizers | Psychology | Kernel Machines
- There are many neuron types and many network types

### The McCulloch-Pitts Artificial Neuron - The first model of an artificial neuron
- ![McCulloch-Pitts Neuron](../imgs/McC-P-Neuron.png)
  - Where x<sup>exc</sup> are the binary inputs, x<sup>inh</sup> are the inhibitory binary inputs, w is the weight for all excitatory inputs
  - z is the combination function (sum of weights by inputs), &phi; is the activation function, &theta; is the threshold, and y is the output
- OR neuron
  - ![OR Neuron](../imgs/OR-Neuron.jpg)
  - where &phi; is: z &geq; &theta;
- AND neuron (with variations)
  - ![AND Neuron](../imgs/AND-Neuron.jpg)
  - where &phi; is: z &geq &theta;
- Various other logical neurons
  - ![Various logical neurons](../imgs/Various-Neurons.png)
- XOR is a unique problem
  - ![XOR Neurons](../imgs/XOR-Neuron.jpg)
  - Where z is the sum of the excitatory inputs times their weights, and &phi; is 1 if z &geq; &theta; AND x<sup>inh</sup> = 0
  
### The Rosenblatt Perceptron
#### Neuron/Linear Threshold Unit (LTU)
- ![Rosenblatt's Perceptron](../imgs/Rosenblatt-perc.png)
- n variable inputs x<sub>1..n</sub>
- x are features not instances
  - We have one constant input x<sub>0</sub> = 1 - **Bias Feature**
- n+1 weights w<sub>0..n</sub>
- One input y per LTU - but a network of LTUs may have multiple outputs, one per LTU
  - &phi;<sub>heaviside</sub> = h(z) = 0 or -1 if z < 0 else if z &geq; 0 then 1
  - &phi;<sub>sign</sub> = sgn(z) = -1 if z < 0, 0 if z = 0, 1 if z > 0
- Input and output are typically binary (but not necessarily)
- No special inhibitory input
- **Training goal: Learn w<sub>0..n</sub>**

#### Single Layer Perceptron
- One input layer, one output layer - input neurons just pass through the inputs
- Each neuron in layer l<sub>k</sub> is connected to each neuron in l<sub>k+1</sub>
- Example of a 2 input, 3 output - Multioutput Classifier<br>![Multioutput Classifier example](../imgs/Multiout-classifier.png)

### Perceptron Training
#### Hebb's rule/Hebbian Learning
- Initialize w<sub>i,j</sub> with small random numbers [-1,1]
- For each training instance:
  - Calculate the output with the current weights
  - Update weights if the prediction was wrong
  - w<sub>i,j</sub> = w<sub>i,j</sub> + &alpha;(y<sub>j</sub> - y'<sub>j</sub>)x<sub>i</sub>
- Continue until Convergence (for linearly separable classes)/Error Threshold/Fixed number of iterations

#### Decision boundary
- Models like SVM try to find the optimal decision boundary - a perceptron just finds *some* decision boundary
- Single layer perceptrons only learn linear decision boundaries
- Only can converge on linearly separable classes
- Poor performance generally and limited in what they can learn - noise can make data non-linearly separable
- **Good for online learning**

### MultiLayer Perceptrons
- One passthrough input layer, one or more hidden layers of LTUs, one output layer of LTUs
- Input layer **and** hidden layer include bias neuron
- 2 or more hidden layers = deep neural network
- With one hidden layer we can model any mathematical function

#### Activation functions
- Originally **Sigmoid/Logistic functions** used: &sigma;(z) = 1/(1+e<sup>-z</sup>) - most similar to biological neurons
- **Hyperbolic Tangent Function:** tanh(z) = (e<sup>z</sup>-e<sup>-z</sup>)/(e<sup>z</sup>+e<sup>-z</sup>)
- **Rectified Linear Unit function:** ReLU(z) = max(0,z) 
  - **Use for hidden layers - fast computation, gradient descent doesn't get stuck on plateaus**
- Softmax for output layer (if classes are mutually exclusive)
- There is no activation function used for regression

#### Intuition - SLP vs MLP
- Single Layer Perceptron<br>![SLP Intuition](../imgs/SLP-Intuition.png)
- Multi Layer Perceptron<br>![MLP Intuition](../imgs/MLP-Intuition.png)

#### Practical Issues
- Designing Neural Networks is an art, not a science
- Number of hidden layers
  - 0 (i.e. one layer of LTUs) -> linearly separable functions
  - 1 -> Any function for continuous mapping from one finite space to another
  - 2 -> Any decision boundary
- More Layers? Our model would move from abstract to complex
  - The less neurons the faster training - more neurons leads to longer training
  - Reduction in effectiveness of backpropagation/unstable gradient descent/more local minima
- Number of Neurons
- Number of input Neurons |N<sup>x</sup>| = *Number of features*
- Number of output Neurons |N<sup>y</sup>| = *Number of (dummy)classes (or 1)*
- Number of Hidden Neurons |N<sup>hid</sup>|
  - Too many: overfitting & long training | Too few: underfitting
  - ![N<sup>hid</sup> calculation](../imgs/Nhid.png)
  - Where |D| is the number of instances in training data, and &alpha; [5..10] (arbitrarily selected scaling factor)

#### Feature Engineering
- With binary inputs - use -1 and 1 not 0 - better performance
- For categorical inputs, encode as dummy variables (-1 or 1)
- For categorical outputs, use softmax

### Training an MLP
#### Gradient
- E<sub>total</sub>&geq;0
- w<sub>j,i</sub> &in; R
- **Update rule for MLP:**
  - ![MLP update rule](../imgs/MLP-update-rule.png)
  - Challenge though - many w's in several layers, and there can be several points of error
- How can we find the optimal weights?

#### Forward Propagation
- Calculate outputs, and errors

#### Backpropagation
- How much does a change in w<sub>j,i</sub> affect the total cost function i.e. the total error
- Adjust weights in a way which minimises the cost
- The algorithm:
  - Present example *x* to the input layer and propagate it through the network.
  - Let y = (y<sub>1</sub>,...,y<sub>m</sub>) be the output vector, and let t(x) = (t<sub>1</sub>,...,t<sub>m</sub>) be the target vector
  - For each output neuron, calculate its responsibility, &delta;<sub>i</sub><sup>(1)</sup>, for the network error: &delta;<sub>i</sub><sup>(1)</sup> = y<sub>i</sub>(1-y<sub>i</sub>)(t<sub>i</sub>-y<sub>i</sub>)
  - For each hidden neuron calculate its responsibility, &delta;<sub>j</sub><sup>(2)</sup>, for the network's error (using the output neurons responsibility):<br>![hidden responsibility](../imgs/Hidden-Reponse.png)
  - Update the weights using the following formulas, where &eta; is the learning rate:<br>![output weight](../imgs/output-weight.png) Where h<sub>j</sub> is the output of the j-th hidden neuron<br>![hidden weight](../imgs/hidden-weight.png) Where x<sub>k</sub> is the value of the k-th attribute
  - Repeat until termination criteria have been satisfied.
- Use delta rule for convenience.

#### Issues of Backpropagation
- Happens every epoch - very expensive
- The more layers, the less effective backpropagation becomes.

### The Role of TF-IDF (Term Frequency-Inverse Document Frequency) or ML

#### Document Clustering/Classification
- Typical ML workflow
  - Transform data, use with ML learning algorithm
  - Examples of data transformation: Encode Nationality as a dummy variable, ignore gender, normalize numbers
- How do we do this with documents?
  - Transform documents into data matrix..?
- **Document-Term Frequency Matrix**
  - In the Corpus *D* with *n* documents *d<sub>i=1..n</sub>* and a vocabulary of *m* terms *t<sub>j=1..m</sub>*, each document *d<sub>i</sub>* is represented as vector of term-document weights *d<sub>i</sub> = {w<sub>i,1</sub>,...,w<sub>i,m</sub>}*
- How to find weights?
  - Boolean - most simple - boolean weights (doesn't capture the degree of true/false)
  - Term frequency(tf) - better - *w<sub>i,j</sub> = tf(d<sub>i</sub>,t<sub>j</sub>)*
  - **TF-IDF - The best** - *w<sub>i,j</sub> = tfidf(d<sub>i</sub>,t<sub>j</sub>,D)*

#### TF-IDF 
- More advanced weighting scheme
- ![TF-IDF calculation](../imgs/TFIDF.png)
  - Where n is the number of documents in the corpus, and |d<sub>t<sub>j</sub></sub>| is the number of docs with term t<sub>j</sub>
  - Note: Log is base 10
